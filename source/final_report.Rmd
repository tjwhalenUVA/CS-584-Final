---
title: "CS 584 Final - Fall 2017"
author: "Jake Whalen"
date: "December 11, 2017"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    toc_depth: 6
      # word_document: 
      #   fig_caption: yes
      #   toc: yes
      #   toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source('_Packages.R')
source('_Data.R')
source('_EDA.R')
source('_ModelResults.R')
```

# Executive Summary
This final project was developed for the GMU CS 584 Spring lecture course at Engility. 
The goal was to choose an interesting data set and apply the data mining techniques learned in class to the data. 
There was an additional goal to go above an beyond what was done in class. 
This was accomplished in several ways during the completion of this project. 

The data selected was from [Kaggle](https://www.kaggle.com/primaryobjects/voicegender) and contains over 3,000 pre-processed audio samples. 
Each sample contains a label for the gender of the voice in the clip. 
The data was stored in a CSV file and then loaded into both R and Python. 
R was used for early exploratory data analysis. 
The data was plotted and run through statistical tests to better understand the underlying distributions. 
Python was used for applying the data mining techniques learnined in class. 
The [scikit-learn](http://scikit-learn.org/stable/) package was used for all of this work. 
It was used previously for all three homeworks so the application of the package was fast and easy for the analyst. 
The results of the models in python were exported into Excel files and then imported into R for reporting. 
The final presentation and report were written in R Markdown for instant reproducibility. 
Making a slight change to the model parameters would automatically update the results displayed in the presentation and report. 

The final results were encouraging. 
The worst model acheived over 80% classification accuracy while the best reached 98%. 
All models did better classifying Male voices. 
The better models acheived higher female classification accuracy. 
In the end it was a difficult choice between 3 different models. 
The best model did not acheive the highest rank in all metrics used for evaluation but it was conistently in the top 3 models for every metric.  

# Motivation
Using machine learning techniques to understand human audio is a trending topic. 
Look no further then the in home personal assistants being sold by companies like Apple (Siri), Google (Google Home), and Amazon (Alexa). 
Answering a question posed by a human however can be quite difficult and would require massive amounts of data and a powerful computer to process and train models. 
This project reduces the scope to a more managelable level. 

This project attempts to develop methods that can classify the gender of a speaker. 
While not as impressive as responding to a question with an answer this project will introduce how different features of a voice can help group the voice into one or more classes. 
These features could be used for more then gender classification. 
This project could potentially be extended to classify the speakers mood or age group. 
Overall this project was motivated by a curiosity to see how well a model could interpret a speakers voice. 

# Data
The data was downloaded from [Kaggle](https://www.kaggle.com/primaryobjects/voicegender) and stored in a CSV file. 
The data consisted of more then 3,000 gender labelled records. 
Each audio clip was pre-processed in R and the transformed into 20 features by the original poster. 
The features included measures such as mean frequency, skewness, spectral entropy and the average of the fundamental frequency measured across the acoustic signal.
The data values all consisted of floating point values.
The features had ranges from 0 up to 1,300 and thus might require som transformations when passed into a model.
There were no missing values which allowed th analyst to concentrate on the models and there results rather then preparing the data. 

# Approach

Three main steps were used to take the raw data and produce models that would accuractely classify a voice by the gender. 
First the method known as exploratory data analysis (EDA) was used to investigate the underlying data distributions. 
Second the data was passed to various algorithms and trained models to then make predictions on a test set of data. 
Third, and finally, the model results were compared using a variety of metrics and a best overall model was chosen.

# EDA

The first step in building a gender classifier was to inspect the data. 
This was acheived mostly through data visualization with a few statistical tests.
The main idea of a visualization is to gain insight into the data. 
Rather then view the raw numbers in a table, a visualization can quickly tell a story.

#### Classes
The first attribute in the data that was inspected was the label. 
This feature is what every model built in step 2 will try to determine and thus may well be the most important feature in the data.
In this project the label consists of two values, male and female.
The graph below shows the count of each label.

```{r, fig.height=3, fig.width=3, fig.align='center'}
genderCount
```

What is important to note is that the classes are evenly distributed. 
There are an equal number of male anf female voice recordings in the dataset.
Thus a 'dumb classifier' that predicts male every time would be correct 50% of the time.
In step 2 this will be used as the threshold to measure wether a model is of any use or not.
A model that acheives less then 50% gender classification accuracy should be discarded. 

#### Visualize

The next step after inspecting the class counts was to visualize the data through graphs.
Since every feature is numeric each one could be inspected individually and against one another.
This step included a lot of trial and error as the analyst atempted to generate plots that would give insight into the data.

###### Distributions

The distribution plots below show each feature broken out by the gender.
The interesting features are the ones where there is very little overlap in the female and male population.
These include the Inter-quartile range (IQR), Mean Fundamental Frequency (meanfun), Lower Quartile (Q25), and Spectral Entropy (sp.ent). 

```{r, fig.height=6, fig.width=8, fig.align='center'}
allDensity
```

The features with non-overlapping distributions give insight into the differences between male and female voices. 
Looking at the IQR densities it is obvious that the females have a lower range then males normally do.
The opposite can be said about the meanfun where females are generally higher then males in this measure.
Knowing this it would be expected for these variables to play an important role in classifying a voices gender.
If the voice has a high IQR and low meanfun then it is most likely a males.

###### Boxplot

The boxplot accomplishes the same task as the density plot for the most part. 
The purpose of making one and including it in this report however was to show which features did not conform to a distribution of any kind very well. 
These would be any feature with a lot of points in its chart. 
The points in a boxplot represent outliers in the data. 
Examples from the plot below include the Kurtosis (kurt), Max Fundamental Frequency (maxfun), and the Skew (skew) just to name a few.

```{r, fig.height=7, fig.width=7, fig.align='center'}
allBoxPlot
```

Recognizing that these features contain outliers means they may need to be transformed in order to be useful.
The outliers may be due to multimodal data or a wide range of values.
Both of which can be fixed by applying a transformation on the data (i.e. log).
Transforming the data will be done during the model building process to see how it may improve results.

###### Heatmap

The heatmap with the overalid correlation values shows any strong positive or negative relationships amongst the features.
There are a few interesting relationships that show very strong correlations between features. 
The first is between the range of the dominant frequency (dfrange) and the max of the dominant frequency (maxdom).
This relationship is understandable because the range would include the max in its calculation (max - min).
The second relationship to inspect is between kurtosis and skew.
Again, this may be expected due to the nature of the features. 
SKew is the measure of the lack of symmetry and kurtosis measures wether the data is heavily tailed or not.

```{r, fig.height=7, fig.width=7}
ggheatmap
```

Within each of the two strongly correlated relationships mentioned a feature can be removed and very little if any information is lost from the datset. 
The purpose of this excercise is to determine which features could be dropped to reduce dimensionality. 
Reducing dimensionality may lead to better results when evaluating models.

###### Scatter Plot

The scatter plot below is another example of how to inspect relationships between features.
The plot shows every feature plotted on the y-axis against the meanfun on the x-axis of every plot.
The meanfun was chosen because it was one of the features from the density plot that best seperated the data. 
Plotting it against other features would hopefully show a combination that further separated the data.
It did to some degree as evidenced by the fairly well separated blue and pink clusters in each graph.
This chart was replicated using each of the strongly separated individual features on the x-axis. Each one resulted in a similar looking graph like the one below.

```{r, fig.height=7, fig.width=7}
oneVall
```

#### Statistical Testing

```{r}
tTestDF
```



# Classification

#### Machine Learning

###### Python

###### Scikit Learn

#### K-Nearest Neighbors

Confusion Matrix

```{r, fig.height=3, fig.width=4, fig.align='center'}
knn_cm + theme(legend.position = 'none')
```

Cross Validation Results

```{r, fig.height=3, fig.width=5, fig.align='center'}
knn_score.graph
```


#### Decision Tree

Confusion Matrix

```{r, fig.height=3, fig.width=4}
dt_cm + theme(legend.position = 'none')
```

Feature Importance

```{r, fig.height=3, fig.width=4}
dt_feature.graph
```

#### Support Vector Machine

Confusion Matrix

```{r, fig.height=3, fig.width=4}
svm_cm + theme(legend.position = 'none')
```


```{r, fig.height=3, fig.width=4}
svm_result.graph
```

#### Logistic Regression

Confusion Matrix

```{r, fig.height=3, fig.width=4}
lr_cm + theme(legend.position = 'none')
```


```{r, fig.height=3, fig.width=4}
lr_result.graph
```

#### K-Nearest Neighbors (PCA)

Confusion Matrix

```{r, fig.height=3, fig.width=4}
knn.pca_cm + theme(legend.position = 'none')
```

Cross Validation Results

```{r, fig.height=3, fig.width=5}
knn.pca_score.graph
```

#### Random Forest

Confusion Matrix

```{r, fig.height=3, fig.width=4}
rf_cm + theme(legend.position = 'none')
```


```{r, fig.height=3, fig.width=4}
rf_result.graph
```

#### Support Vector Machine (PCA)

Confusion Matrix

```{r, fig.height=3, fig.width=4}
svm.pca_cm + theme(legend.position = 'none')
```


```{r, fig.height=3, fig.width=4}
svm.pca_result.graph
```

#### Logistic Regression (Normalized)

Confusion Matrix

```{r}
lr.norm_cm + theme(legend.position = 'none')
```

```{r}
lr.norm_result.graph
```




# Conclusions

#### Criteria

```{r fig.height=4, fig.width=8}
model_criteria
```

#### ROC

```{r fig.height=6, fig.width=6}
model_roc
```

#### Fitting Times

```{r fig.height=5, fig.width=5}
fit_time.graph + theme(legend.position='none')
```

#### Scoring Times

```{r fig.height=5, fig.width=5}
score_time.graph + theme(legend.position='none')
```





