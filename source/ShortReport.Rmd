---
title: "CS 584 Final Project"
author: "Jake Whalen"
date: "December 14, 2017"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source('_Packages.R')
source('_Data.R')
source('_EDA.R')
source('_ModelResults.R')
```

# Description

## Executive Summary
This final project was developed for the GMU CS 584 Spring lecture course at Engility. 
The goal was to choose an interesting data set and apply the data mining techniques learned in class to the data. 
There was an additional goal to go above an beyond what was done in class. 
This was accomplished in several ways during the completion of this project. 

The data selected was from [Kaggle](https://www.kaggle.com/primaryobjects/voicegender) and contains over 3,000 pre-processed audio samples. 
Each sample contains a label for the gender of the voice in the clip. 
The data was stored in a CSV file and then loaded into both R and Python. 
R was used for early exploratory data analysis. 
The data was plotted and run through statistical tests to better understand the underlying distributions. 
Python was used for applying the data mining techniques learned in class. 
The [scikit-learn](http://scikit-learn.org/stable/) package was used for all of this work. 
It was used previously for all three homework assignments so the application of the package was fast and easy for the analyst. 
The results of the models in python were exported into Excel files and then imported into R for reporting. 
The final presentation and report were written in R Markdown. 
This meant that making a slight change to the model parameters would automatically update the results displayed in the presentation and report. 

The final results were encouraging. 
The worst model achieved over 80% classification accuracy while the best reached 98%. 
All models did better classifying Male voices. 
The better models achieved higher female classification accuracy. 
In the end it was a difficult choice between 3 different models. 
The best model did not achieve the highest rank in all metrics used for evaluation but it was consistently in the top 3 models for every metric.  

## Motivation
Using machine learning techniques to understand human audio is a trending topic. 
Look no further then the in home personal assistants being sold by companies like Apple (Siri), Google (Google Home), and Amazon (Alexa). 
Answering a question posed by a human however can be quite difficult and would require massive amounts of data and a powerful computer to process and train models. 
This project reduces the scope to a more manageable level. 

This project attempts to develop methods that can classify the gender of a speaker. 
While not as impressive as responding to a question with an answer this project will introduce how different features of a voice can help group the voice into one or more classes. 
These features could be used for more then gender classification. 
This project could potentially be extended to classify the speakers mood or age group. 
Overall this project was motivated by a curiosity to see how well a model could interpret a speakers voice. 

## Data
The data was downloaded from [Kaggle](https://www.kaggle.com/primaryobjects/voicegender) and stored in a CSV file. 
The data consisted of more then 3,000 gender labelled records. 
Each audio clip was pre-processed in R and the transformed into 20 features by the original poster. 
The features included measures such as mean frequency, skewness, spectral entropy and the average of the fundamental frequency measured across the acoustic signal.
The data values all consisted of floating point values.
The features had ranges from 0 up to 1,300 and thus might require some transformations when passed into a model.
There were no missing values which allowed the analyst to concentrate on the models and there results rather then preparing the data. 


# Methods

## EDA

The first step in building a gender classifier was to inspect the data. 
This was achieved through data visualization and statistical tests.
The main idea of a visualization is to gain insight into the data. 
Rather then view the raw numbers in a table, a visualization can quickly tell a story.

The first feature in the to be inspected was the label. 
This feature is what every model built in the classification step will try to determine.
Thus the label may well be the most important feature in the data.
In this project the label consists of two values, male and female.
The graph below shows the count of each label.

```{r, fig.height=2, fig.width=2}
genderCount
```

What is important to note is that the classes are evenly distributed. 
There are an equal number of male and female voice recordings in the data set.
Thus a 'dumb classifier' that predicts male every time would be correct 50% of the time.
In step 2 this will be used as the threshold to measure whether a model is of any use or not.
A model that achieves less then 50% gender classification accuracy should be discarded. 

The next step after inspecting the class counts was to visualize the data through graphs.
Since every feature is numeric each one could be inspected individually and against one another.
This step included a lot of trial and error attempting to generate plots that would give insight into the data.

Multiple plots were genereated such as density, box, heat map, and 2D/3D scatter plots.
The density and boxplots helped show how each feature was distributed and potentially split the gender classes apart. 
The scatter plots helped to show the interaction between multiple features. 
In some cases two features would help to split the genders apart to make classification easier.
The heat map generated included correlations between features.
The correlations were used to remove any features where there was a perfect positive or negative correlation. 
Removing one of the features from this pair would not cause the data to lose any information while reducing the dimensionality.
After inspecting the visualizations and removing some of the features that supplied dulicate information the modeling phase began.

## Models

The modeling phase consisted of two steps using four different algorithms.
The first step used only the raw data with no transformation.
The models included K-Nearest Neighbors (KNN), Decision Trees, Support Vector Machines (SVM), and Logistic Regression (Log R). 
The second step applied a transformation on the data prior to fitting the model.
In both steps the models were fit using a grid search mechanism which allowed parameters to be varied and the best model chosen based on the cross-validation results.

### K-Nearest Neighbors

The first algorithm used was KNN.
The parameters used to attempt to find the best KNN model included algorithm type, number of neighbors, distance metric, and weights.
The first model fitted using raw data found that `r best_params(knn)` reslted in the best cross-validation scores. 
After predicting genders on the test data the model resulted in an accuracy of `r model_accuracy(knn)`.

Next a transformation was used on the data known as Principal Component Analysis (PCA).
The idea behind PCA is to reduce the dimensionality data and find a set of vectors to capture a majority of the variance in fewer features.
After applying PCA and fitting the KNN model it was determined that 9 components maximized the mean cross-validation test scores.
As for the other parameters `r best_params(knn.pca)` reslted in the best  scores.
Overall the KNN model with the PCA transform resulted in an accuracy score of `r model_accuracy(knn.pca)`.

### Decision Tree

The next model used was a decision tree.
The decision tree is simple to understand. 
At every node the tree makes a decison to split the data based on one features value.
Again a grid search was used to find the parameters that build the best tree for the data.
It ended up that `r best_params(dt)` generated the best tree with an accuracy of `r model_accuracy(dt)`.

Rather then transform the data since the original model did so well the choice was made to apply more trees and take a majority vote.
This method is also known as a Random Forest classifier.
The Random Forest classifier found that `r best_params(rf)` reslted in the best  scores.
It's accuracy when classifying the test data was `r model_accuracy(rf)`.

### Support Vector Machine

Next a SVM was created to classify the data.
This algorithm attempts to split the data multiple ways through equations and perfectly divide the genders into their on groups.
It was found that by altering the penalty parameter C for SVM the model could acheive beter results.
The SVM classifier found that `r best_params(svm)` reslted in the best  scores.
It's accuracy when classifying the test data was `r model_accuracy(svm)`.

Similar to KNN the SVM model then was used with PCA transformed data to attempt to improve the baseline results.
The performance was best when 13 principal components were used to fit the model.
The SVM classifier with PCA data found that `r best_params(svm.pca)` reslted in the best  scores.
It's accuracy when classifying the test data was `r model_accuracy(svm.pca)`.

### Logistic Regression

Lastly, a model was built using Logistic Regression.
The model was fit first with only the raw data.
The Log Regression model accepted numerous parameters in python.
TO acheive beter results this model varied the inverse of regularization strength (C), intercept fit, and a penalty value.
A grid search found that `r best_params(lr)` reslted in the best  scores.
It's accuracy when classifying the test data was `r model_accuracy(lr)`.

Rather then apply a PCA transform which resulted in no change in accuracy for the Log Regression model a normalization was applied to the data.
Each columns values were transformed so that the max value was set to 1 and minimum value was 0.
This resulted in the model accuracy decreasing to `r model_accuracy(lr.norm)`.
It seemed that the Log Regression model performed best on the raw data.

# Results

Best model

```{r, fig.height=3, fig.width=8}
model_criteria
```

```{r, fig.height=3, fig.width=8}
model_roc
```

# Conclusions

Lesons learned
Positives
Negatives
Moving foward